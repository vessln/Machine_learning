%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression








diabetic_data = pd.read_csv("data/diabetic_data.csv", na_values = ["?"])





diabetic_data.head()


diabetic_data.columns


diabetic_data.info()





diabetic_data.readmitted.unique()


diabetic_data.readmitted.value_counts()


diabetic_data.race.value_counts(dropna=False)


diabetic_data.gender.value_counts(dropna=False)


diabetic_data.age.value_counts(dropna=False)


# normalise age data (%):
diabetic_data.age.value_counts(dropna=False) / len(diabetic_data) * 100


# this column have high cardinality
diabetic_data.discharge_disposition_id.value_counts(dropna=False)


diabetic_data.metformin.value_counts(dropna=False)


diabetic_data.patient_nbr


# how many records there are for one patient:
diabetic_data.patient_nbr.nunique() / len(diabetic_data)


# how many visits patients have:
diabetic_data.patient_nbr.value_counts(dropna=False)


diabetic_data.patient_nbr.value_counts().hist(bins=50)
plt.show()





# check the patient that have 40 visits
diabetic_data[diabetic_data.patient_nbr == 88785891].sort_values(by="encounter_id").insulin





attributes = diabetic_data.drop(columns = "readmitted")


# labels:
target = diabetic_data.readmitted





attributes = attributes.drop(columns = ["encounter_id", "patient_nbr"]) 


# main diagnose:
len(attributes[attributes.diag_1.isna()])


len(attributes[attributes.diag_2.isna()])


len(attributes[attributes.diag_3.isna()])


len(attributes[attributes.weight.isna()]) # too many Nan values


attributes = attributes.drop(columns = "weight") 


attributes = attributes.drop(columns = ["payer_code", "medical_specialty"]) 


attributes.dropna()


attributes.time_in_hospital.hist(bins = "fd")
plt.xlabel("days")
plt.ylabel("people count")
plt.show()


attributes.num_medications.hist(bins = 40)
plt.xlabel("number of medications")
plt.ylabel("people count")
plt.show()


# errors like:
0.1 + 0.2 == 0.3, 0.1 + 0.2


10000000000000000.0 + 1 == 10000000000000000.0








# Z-score (standartization)
z_score = (attributes.num_medications - attributes.num_medications.mean()) / attributes.num_medications.std()


z_score.hist(bins = 40)
plt.xlabel("-/+ value std from the mean")
plt.ylabel("frequency")
plt.show()
z_score


# Min-Max scaling:
norm = (attributes.num_medications - attributes.num_medications.min()) / (attributes.num_medications.max() -  attributes.num_medications.min())


plt.hist(norm, bins = 40)
plt.show()





# get_dummies() transforms categorical variables into numeric ones by creating new binary columns for each category:
pd.get_dummies(attributes[["metformin"]]).astype(int)


attributes.metformin.replace({"No": -99, "Down": -1, "Up": 1, "Steady": 0})


# to reduce the number of columns:
attributes = pd.get_dummies(attributes, drop_first=True)


attributes


# Scaling:
scaler = MinMaxScaler()


scaler.fit(attributes)


scaler.transform(attributes).max(axis = 0)


attributes = scaler.transform(attributes)


attributes.shape


attributes.dtype


encoder = OneHotEncoder()


lencoder = LabelEncoder()





model = LogisticRegression()


model.fit(attributes, target)


# classification accuracy:
model.score(attributes, target)



