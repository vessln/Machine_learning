%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier

from sklearn.datasets import make_blobs

from sklearn.model_selection import train_test_split

from sklearn.cluster import KMeans





for chunk in pd.read_csv("../5.Support_Vector_Machines/data/pulsar_stars.csv", chunksize=1000):
    print(chunk[" Mean of the integrated profile"].max(), chunk[" Mean of the integrated profile"].min())





svm = LinearSVC()


# SGDClassifier is equivalent to LinearSVC:
sgd = SGDClassifier(max_iter = 1000)


for chunk in pd.read_csv("../5.Support_Vector_Machines/data/pulsar_stars.csv", chunksize=1000):
    X = chunk.drop(columns = "target_class")
    y = chunk["target_class"]
    print(X.shape)

    # svm.partial_fit(X, y) -> this is impossible
    
    sgd.partial_fit(X, y, classes = y.unique())
    # sgd.fit(X, y) # reset learning in each iteration


sgd.coef_























def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))


def kmeans(X, k, max_iters=100):
    np.random.seed(42)
    
    # 1. Randomly initialize centroids
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]
    print(f"Initial centroids:\n{centroids}\n")
    
    for i in range(max_iters):
        print(f"Iteration {i+1}:")
        
        # 2. Assign each point to the nearest centroid
        clusters = [[] for _ in range(k)]
        labels = []
        for point in X:
            distances = [euclidean_distance(point, centroid) for centroid in centroids]
            closest_centroid = np.argmin(distances)
            clusters[closest_centroid].append(point)
            labels.append(closest_centroid)
        
        # 3. Calculate new centroids as the mean of assigned points
        new_centroids = np.array([np.mean(cl, axis=0) if cl else centroids[i] for i, cl in enumerate(clusters)])
        print(f"New centroids:\n{new_centroids}\n")
        
        # If centroids do not change, we have converged
        if np.all(centroids == new_centroids):
            print("Convergence reached.")
            break
        
        centroids = new_centroids

    colors = ['r', 'g', 'b']
    for j, cluster in enumerate(clusters):
        cluster_points = np.array(cluster)
        if len(cluster_points) > 0:
            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[j])
    
    plt.scatter(centroids[:, 0], centroids[:, 1], color='k', marker='x', s=150, label='Centroids')
    plt.title("Clusters and Centroids")
    plt.xlabel("feature 1")
    plt.ylabel("feature 2")
    plt.show()
    
    return labels


X = np.array([[6, 7], [1, 3], [2.2, 4], [1.8, 3.1], [6, 7.9], [11, 13], [7.2, 10.2], [7, 8], [3.3, 1.7], [13, 11.6]])
kmeans(X, 3)





attributes, clusters = make_blobs(n_samples = 1000)


plt.scatter(attributes[:, 0], attributes[:, 1], c = clusters, s = 5)
plt.show()


attributes_train, attributes_test, clusters_train, clusters_test = train_test_split(attributes, clusters, test_size = 0.1, stratify = clusters)


plt.scatter(attributes_test[:, 0], attributes_test[:, 1], c = clusters_test, s = 5)
plt.show()


kmeans = KMeans(n_clusters = 3, init = "random", n_init="auto")


kmeans.fit(attributes_train)


train_predictions = kmeans.predict(attributes_train)


test_predictions = kmeans.predict(attributes_test)


plt.scatter(attributes_train[:, 0], attributes_train[:, 1], c = train_predictions, s = 5)
plt.title("Train predictions")
plt.show()


plt.scatter(attributes_test[:, 0], attributes_test[:, 1], c = test_predictions, s = 5)
plt.title("Test predictions")
plt.show()








kmeans1 = KMeans(n_clusters = 5, init = "random", n_init="auto")


attributes1, clusters1 = make_blobs(n_samples = 500, cluster_std=3)


plt.scatter(attributes1[:, 0], attributes1[:, 1], c = clusters1)
plt.show()


kmeans1.fit(attributes1)


plt.scatter(attributes1[:, 0], attributes1[:, 1], c = kmeans1.predict(attributes1))
plt.show()





kmeans2 = KMeans(n_clusters = 3, init = "random")


attributes2, clusters2 = make_blobs(n_samples = 500)


skewed_attributes = attributes2 @ np.array([
                                    [1, 0.5],
                                    [0.7, 1],
                                    ])


plt.scatter(skewed_attributes[:, 0], skewed_attributes[:, 1], c = clusters2)
plt.show()


kmeans2.fit(skewed_attributes)


kmeans2.cluster_centers_


plt.scatter(skewed_attributes[:, 0], skewed_attributes[:, 1], c = kmeans2.predict(skewed_attributes))
plt.scatter(kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1], c = "black", marker='x')
plt.show()


kmeans3 = KMeans(n_clusters = 3, init = "k-means++")


kmeans3.fit(skewed_attributes)


plt.scatter(skewed_attributes[:, 0], skewed_attributes[:, 1], c = kmeans3.predict(skewed_attributes))
plt.scatter(kmeans3.cluster_centers_[:, 0], kmeans3.cluster_centers_[:, 1], c = "black", marker='x')
plt.show()


kmeans3 = KMeans(n_clusters = 9, init = "k-means++")


kmeans3.fit(skewed_attributes)


plt.scatter(skewed_attributes[:, 0], skewed_attributes[:, 1], c = kmeans3.predict(skewed_attributes))
plt.scatter(kmeans3.cluster_centers_[:, 0], kmeans3.cluster_centers_[:, 1], c = "black", marker='x')
plt.show()


kmeans4 = KMeans(n_clusters = 3, init = [[1, 3], [-2, 5], [3.3, 7]])


kmeans4.fit(attributes)














# the value of loss function:
kmeans3.inertia_





data = {"n_clusters": [], "inertia_values": []}

for k in range(1, 11):
    kmeans_test = KMeans(n_clusters = k)
    kmeans_test.fit(attributes)
    data["n_clusters"].append(k)
    data["inertia_values"].append(kmeans_test.inertia_)


data = pd.DataFrame(data)
plt.scatter(data.n_clusters, data.inertia_values, color="r")
plt.plot(data.n_clusters, data.inertia_values)
plt.title("Ewbol method visualisation")
plt.xlabel("Number of clusters (k)")
plt.ylabel("Inertia")
plt.show()


-data.inertia_values.diff()



